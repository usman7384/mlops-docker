# -*- coding: utf-8 -*-
"""sentiment_analysis_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rkmQ7RhBKk-oyGODSu1pCROqq8vmu_2p
"""
import tensorflow as tf
from tensorflow.keras import activations, optimizers, losses
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification


import pandas as pd
import numpy as np


df=pd.read_csv('/content/drive/MyDrive/amazon_alexa.tsv',sep='\t')

df.head()

df.shape

df['feedback'].value_counts()

"""1 indicates positive review whereas 0 indicates negative review.

As we can see, our dataset has more positive reviews than negative reviews.

We have to address this class imbalance before we train our model.

We do this by sub sampling the positive reviews class to select the same number of positive reviews as the negative reviews.
"""

df[df['feedback']==0].shape

df[df['feedback']==1].shape

df_negative=df[df['feedback']==0]
df_positive=df[df['feedback']==1]

df_positive_downsampled=df_positive.sample(df_negative.shape[0])
df_positive_downsampled.shape

df_balanced=pd.concat([df_positive_downsampled,df_negative],axis=0)
df_balanced.shape

df_balanced['review_length']=df_balanced['verified_reviews'].apply(lambda x: len(x.split()))

df_balanced.shape

x=df_balanced['verified_reviews'].to_list()

y=df_balanced['feedback'].to_list()

MODEL_NAME = 'distilbert-base-uncased'
# hyper parameter
MAX_LEN = 50

review = "I liked the service a lot! Keep it up!"

tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)

inputs = tokenizer(review, max_length=MAX_LEN, truncation=True,padding='max_length')

tokenized_words=tokenizer.convert_ids_to_tokens(inputs["input_ids"])
print(tokenized_words)

def construct_encodings(x, tokenizer, max_len, trucation=True, padding=True):
    return tokenizer(x, max_length=max_len, truncation=trucation, padding=padding)

encodings = construct_encodings(x, tokenizer, max_len=MAX_LEN)

def construct_tfdataset(encodings, y=None):
    if y:
        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))
    else:
        return tf.data.Dataset.from_tensor_slices(dict(encodings))

tfdataset = construct_tfdataset(encodings, y)

TEST_SPLIT = 0.2
VAL_SPLIT=0.2
BATCH_SIZE = 32


train_size = int(len(x) * (1-TEST_SPLIT))
val_size=int(len(x) * (VAL_SPLIT))

tfdataset = tfdataset.shuffle(len(x))

tfdataset_test = tfdataset.skip(train_size)


tfdataset_rest = tfdataset.take(train_size)


tfdataset_train=tfdataset_rest.skip(val_size)
tfdataset_val=tfdataset_rest.take(val_size)

tfdataset_train = tfdataset_train.batch(BATCH_SIZE)
tfdataset_val = tfdataset_val.batch(BATCH_SIZE)
tfdataset_test = tfdataset_test.batch(BATCH_SIZE)

# hyper parameter
N_EPOCHS = 10

MODEL_NAME = 'distilbert-base-uncased'

model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
optimizer = optimizers.Adam(learning_rate=3e-5)
loss = losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

model.summary()

history=model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS,validation_data=tfdataset_val,verbose=1)

benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE,verbose=1)

def create_predictor(model, model_name, max_len):
  tokenizer = DistilBertTokenizer.from_pretrained(model_name)
  def predict_proba(text):
      x = [text]

      encodings = construct_encodings(x, tokenizer, max_len=max_len)
      tfdataset = construct_tfdataset(encodings)
      tfdataset = tfdataset.batch(1)

      preds = model.predict(tfdataset).logits
      preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()

      return preds[0][1]

  return predict_proba

clf = create_predictor(model, MODEL_NAME, MAX_LEN)
print(clf("The technician was very talented and polite. He completed his work quickly. I loved his work!"))

import pickle
model.save_pretrained('/content/drive/MyDrive/sentiment_analyzer_model')
with open('/content/drive/MyDrive/sentiment_analyzer_model/info.pkl', 'wb') as f:
    pickle.dump((MODEL_NAME, MAX_LEN), f)

